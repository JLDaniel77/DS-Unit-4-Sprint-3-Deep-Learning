{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# Get file and save as text file\n",
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "path_to_file = get_file('shakespeare.txt', url)\n",
    "\n",
    "doc = open(path_to_file, 'rb').read().decode(encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document: 5278325 characters\n"
     ]
    }
   ],
   "source": [
    "# Remove whitespace and make all characters lowercase\n",
    "doc = doc.replace(\"\\r\", \"\")\n",
    "doc = doc.lower()\n",
    "doc = \" \".join(doc.split())\n",
    "print(f\"Length of document: {len(doc)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: 76\n",
      "txt_data_size: 5278325\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split and remove duplicate characters and convert to a list.\n",
    "chars = list(set(doc))\n",
    "\n",
    "num_chars = len(chars)\n",
    "txt_data_size = len(doc)\n",
    "\n",
    "print(\"unique characters:\", num_chars)\n",
    "print(\"txt_data_size:\", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"': 0, '#': 1, 'j': 2, '’': 3, 'u': 4, 'f': 5, 'à': 6, 't': 7, \"'\": 8, ';': 9, '-': 10, ')': 11, '—': 12, 'è': 13, '/': 14, 'w': 15, 'ç': 16, '%': 17, 'n': 18, 'k': 19, 'e': 20, ' ': 21, 'æ': 22, 'œ': 23, 'é': 24, '[': 25, '?': 26, '8': 27, 'i': 28, 'x': 29, '3': 30, '6': 31, ':': 32, 'c': 33, '.': 34, 'â': 35, '\\\\': 36, '|': 37, '5': 38, 'p': 39, 'm': 40, '*': 41, ',': 42, '‘': 43, '7': 44, 'z': 45, 'b': 46, 'a': 47, '(': 48, 's': 49, ']': 50, '0': 51, '”': 52, '&': 53, '_': 54, '1': 55, '“': 56, '2': 57, '@': 58, 'r': 59, 'y': 60, 'î': 61, 'o': 62, '4': 63, 'v': 64, 'ê': 65, '`': 66, 'h': 67, '}': 68, '!': 69, '$': 70, 'l': 71, 'd': 72, 'q': 73, 'g': 74, '9': 75}\n",
      "-----------------------------------------------\n",
      "{0: '\"', 1: '#', 2: 'j', 3: '’', 4: 'u', 5: 'f', 6: 'à', 7: 't', 8: \"'\", 9: ';', 10: '-', 11: ')', 12: '—', 13: 'è', 14: '/', 15: 'w', 16: 'ç', 17: '%', 18: 'n', 19: 'k', 20: 'e', 21: ' ', 22: 'æ', 23: 'œ', 24: 'é', 25: '[', 26: '?', 27: '8', 28: 'i', 29: 'x', 30: '3', 31: '6', 32: ':', 33: 'c', 34: '.', 35: 'â', 36: '\\\\', 37: '|', 38: '5', 39: 'p', 40: 'm', 41: '*', 42: ',', 43: '‘', 44: '7', 45: 'z', 46: 'b', 47: 'a', 48: '(', 49: 's', 50: ']', 51: '0', 52: '”', 53: '&', 54: '_', 55: '1', 56: '“', 57: '2', 58: '@', 59: 'r', 60: 'y', 61: 'î', 62: 'o', 63: '4', 64: 'v', 65: 'ê', 66: '`', 67: 'h', 68: '}', 69: '!', 70: '$', 71: 'l', 72: 'd', 73: 'q', 74: 'g', 75: '9'}\n",
      "-----------------------------------------------\n",
      "5278325\n"
     ]
    }
   ],
   "source": [
    "# One hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"-----------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"-----------------------------------------------\")\n",
    "# Integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in doc]\n",
    "print(len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "iteration = 5\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size / sequence_length) + 0.5)\n",
    "hidden_size = 500\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameters\n",
    "W_xh = np.random.randn(hidden_size, num_chars) * 0.01       # weight input => hidden\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01     # weight hidden => hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size) * 0.01       # weight hidden => output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1))   # hidden bias\n",
    "b_y = np.zeros((num_chars, 1))     # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size, 1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} \n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)):   # t is a \"time step\" and is used as a key(dict)\n",
    "        \n",
    "        xs[t] = np.zeros((num_chars, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state\n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars.\n",
    "        \n",
    "        # softmax\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # softmax (cross-entropy loss)\n",
    "        \n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matricies\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size, 1)\n",
    "    \n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(W_hy.T, dy) + dhnext\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "    data_pointer = 0\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in doc[data_pointer: data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in doc[data_pointer+1: data_pointer+sequence_length+1]]\n",
    "        \n",
    "        if (data_pointer+sequence_length+1 >= len(doc) and b == batch_size-1):\n",
    "            targets.append(char_to_int[\" \"])   # When data doesn't fit, add space(\" \") to the back.\n",
    "            \n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "        \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets)\n",
    "        \n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y],\n",
    "                                     [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                     [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "        data_pointer += sequence_length\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print(f\"iter {i}, loss: {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1))\n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel())\n",
    "        x = np.zeros((num_chars, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print(f\"----\\n {txt} \\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " bdruta \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "predict('b', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u4_s2",
   "language": "python",
   "name": "u4_s2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
